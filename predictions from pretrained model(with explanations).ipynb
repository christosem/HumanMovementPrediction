{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "kernelspec": {
   "name": "python",
   "display_name": "Python (Pyodide)",
   "language": "python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This nodebook explains how to use the pretrained graph neural network and generate predictions. First, the model as used during trainings needs to be defined(**Defining the Model**). Wherafter, the prediction can be made using the pretrained model(**Generate Predictions**). Each section has additional explanations about what the code does and what the input parameters should be. \n",
    "\n",
    "\n",
    "The model in this notebook is a graph based neural network. The graph based approach can directly capture the spatial dynamics of the data by the topology of the graph. The graph is created by converting each grid cell to a node. Each node is connected to its first degree neightbours including the diagonal. Each node contains information about the human occupancy and some contextual information (if a wall, workbench or coffee machine is present in that node). Lastly, to make a sparser graph, nodes that are not visited during the simulation where omitted from the graph during training.\n",
    "\n",
    "The trained model in this case is trained on 1 specific layout (simulation 2). Moreover, the omission of some unvisited nodes and addition of learnable edge weights, make the supplied pretrained model not generalizable to different layouts of the simulation. If you want to train a model on a different setting or with different parameters, the README file in the folder explains the process. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2, TGCN2\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv, ChebConv"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code block defines the specific graph-based model. Later this model will be initialized and the pretrained weights will be loaded. \n",
    "\n",
    "This code block is a direct copy from the library 'pytorch geometric temperal' but instead of importing, it must be explicitly defined in order to work with the pretrained weights. The model must be identical to the model used during training. Any changes to the model during training, must also be implemented here.\n",
    "\n",
    "The model used here (and during training) is called A3T-GCN and is designed by Bai et al. (2021)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class TGCN2(torch.nn.Module):\n",
    "    r\"\"\"An implementation THAT SUPPORTS BATCHES of the Temporal Graph Convolutional Gated Recurrent Cell.\n",
    "    For details see this paper: `\"T-GCN: A Temporal Graph ConvolutionalNetwork for\n",
    "    Traffic Prediction.\" <https://arxiv.org/abs/1811.05320>`_\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        batch_size (int): Size of the batch.\n",
    "        improved (bool): Stronger self loops. Default is False.\n",
    "        cached (bool): Caching the message weights. Default is False.\n",
    "        add_self_loops (bool): Adding self-loops for smoothing. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 batch_size: int,  # this entry is unnecessary, kept only for backward compatibility\n",
    "                 improved: bool = False, cached: bool = False,\n",
    "                 add_self_loops: bool = True):\n",
    "        super(TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.batch_size = batch_size  # not needed\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "        self.conv_z = ChebConv(in_channels=self.in_channels,  out_channels=self.out_channels, K=10, normalization=\"sym\", agrr=\"add\") # other ChebCov parameters possible\n",
    "        self.linear_z = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "        self.conv_r = ChebConv(in_channels=self.in_channels,  out_channels=self.out_channels, K=10, normalization=\"sym\", agrr=\"add\")\n",
    "        self.linear_r = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "        self.conv_h = ChebConv(in_channels=self.in_channels,  out_channels=self.out_channels, K=10, normalization=\"sym\", agrr=\"add\")\n",
    "        self.linear_h = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            # can infer batch_size from X.shape, because X is [B, N, F]\n",
    "            H = torch.zeros(X.shape[0], X.shape[1], self.out_channels).to(X.device)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H):\n",
    "        Z = torch.cat([self.conv_z(X, edge_index, edge_weight), H], axis=2)\n",
    "        Z = self.linear_z(Z)\n",
    "        Z = torch.sigmoid(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H):\n",
    "        R = torch.cat([self.conv_r(X, edge_index, edge_weight), H], axis=2) \n",
    "        R = self.linear_r(R)\n",
    "        R = torch.sigmoid(R)\n",
    "\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R):\n",
    "        H_tilde = torch.cat([self.conv_h(X, edge_index, edge_weight), H * R], axis=2)\n",
    "        H_tilde = self.linear_h(H_tilde)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        return H\n",
    "\n",
    "    def forward(self,X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,\n",
    "                H: torch.FloatTensor = None ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde)\n",
    "        return H\n",
    "\n",
    "\n",
    "class A3TGCN2(torch.nn.Module):\n",
    "    r\"\"\"An implementation THAT SUPPORTS BATCHES of the Attention Temporal Graph Convolutional Cell.\n",
    "    For details see this paper: `\"A3T-GCN: Attention Temporal Graph Convolutional\n",
    "    Network for Traffic Forecasting.\" <https://arxiv.org/abs/2006.11583>`_\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        periods (int): Number of time periods.\n",
    "        improved (bool): Stronger self loops (default :obj:`False`).\n",
    "        cached (bool): Caching the message weights (default :obj:`False`).\n",
    "        add_self_loops (bool): Adding self-loops for smoothing (default :obj:`True`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        periods: int,\n",
    "        batch_size:int,\n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: bool = True):\n",
    "        super(A3TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels \n",
    "        self.out_channels = out_channels \n",
    "        self.periods = periods \n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.batch_size = batch_size\n",
    "        self._setup_layers()\n",
    "\n",
    "    def _setup_layers(self):\n",
    "        self._base_tgcn = TGCN2(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            batch_size=self.batch_size,\n",
    "            improved=self.improved,\n",
    "            cached=self.cached,\n",
    "            add_self_loops=self.add_self_loops)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._attention = torch.nn.Parameter(torch.empty(self.periods, device=device))\n",
    "        torch.nn.init.uniform_(self._attention)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "        Arg types:\n",
    "            * **X** (PyTorch Float Tensor): Node features for T time periods.\n",
    "            * **edge_index** (PyTorch Long Tensor): Graph edge indices.\n",
    "            * **edge_weight** (PyTorch Long Tensor, optional)*: Edge weight vector.\n",
    "            * **H** (PyTorch Float Tensor, optional): Hidden state matrix for all nodes.\n",
    "        Return types:\n",
    "            * **H** (PyTorch Float Tensor): Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H_accum = 0\n",
    "        probs = torch.nn.functional.softmax(self._attention, dim=0)\n",
    "        for period in range(self.periods):\n",
    "\n",
    "            H_accum = H_accum + probs[period] * self._base_tgcn( X[:, :, :, period], edge_index, edge_weight, H)\n",
    "\n",
    "        return H_accum\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared with the model of Bai et al. (2021), this model used has learnable edge weights to capture the spatial dynamics. The trained weights will be loaded in the class 'STGNN_model'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class TemporalGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, periods_in, periods_out, batch_size, num_edges):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "\n",
    "        # initialize learnable edge weights\n",
    "        self.edge_weight = torch.nn.Parameter(torch.full((num_edges,), 1 / 8))\n",
    "\n",
    "        # Attention Temporal Graph Convolutional Cell\n",
    "        self.tgnn = A3TGCN2(in_channels=node_features, out_channels=256, periods=periods_in, batch_size=batch_size)\n",
    "\n",
    "        # Equals single-shot prediction\n",
    "        self.linear = torch.nn.Linear(256, periods_out)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x = Node features for T time steps\n",
    "        edge_index = Graph edge indices\n",
    "        \"\"\"\n",
    "        h = self.tgnn(x, edge_index, self.edge_weight.relu())\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "\n",
    "        return h"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code block handles the prediction process. The class 'STGNN_model' initializes the model as defined above and loads the supplied pretrained weights. To make a prediction, you supply raw data from the simulations (as txts). This class loads this data and handles all the data processing before making a prediction about the future human occuancy using the graph based network. Again, all the inputs and data processing must be the same as during the training phase."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class STGNN_model():\n",
    "    r\"\"\"\"\n",
    "    This class handles the process of generating predictions from a pretrained spatio-temporal graph neural network\n",
    "    following the methodology of 'Graph-Based Approach to Human Movement Prediction in Shared Human-Robot Workspaces' by Casper Dik\n",
    "\n",
    "    Args:\n",
    "        node_features (int): number of node features used in the pretrained model.\n",
    "        periods_in (int): number of input time steps of the pretrained model.\n",
    "        filepath_model (str): the filepath for loading the pretrained PyTorch model.\n",
    "        adj_reduced (numpy array): a numpy array storing the connectivity information (adjacency matrix) of the\n",
    "            reduced graph as defined in the thesis. The adjacency matrix can be produced by running the function adj_matrix\n",
    "            from generate_input_matrices.py, whereafter, the function reduce_graph from reduce_graph.py should be ran to\n",
    "            get the adjacency matrix of graph without the omitted nodes as specified in the thesis methodology.\n",
    "        idx (numpy array): a numpy array containing the indices of the omitted nodes. Can be generated using the\n",
    "            function reduce_graph from reduce_graph.py.\n",
    "        normalize (boolean): a boolean variable that controls whether not to perform z-score normalization on the input and\n",
    "            denormalization on the outputs. If the supplied pretrained model is trained on normalized inputs, the\n",
    "            variable normalize should be set to TRUE.\n",
    "        means (numpy array, optional): a numpy array containing the means of the normalized node features.\n",
    "        stdev (numpy array, optional): a numpy array containing the standard deviations of the normalized node features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features: int, periods_in: int, periods_out: int, filepath_model: str, adj_reduced: np.ndarray, idx: np.ndarray, normalize: bool = True,\n",
    "                 means: np.ndarray = None, stdev: np.ndarray = None):\n",
    "        self.normalize = normalize\n",
    "        self.periods_in = periods_in\n",
    "        self.DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "        self.A, _ = dense_to_sparse(torch.from_numpy(adj_reduced))     # create sparse adjacency matrix\n",
    "        self.idx = idx\n",
    "        self.means = means\n",
    "        self.stdev = stdev\n",
    "        \n",
    "        # intialize the model as defined \n",
    "        self.stgnn = TemporalGNN(node_features=node_features, periods_in=self.periods_in, periods_out=periods_out, batch_size=1, num_edges=self.A.shape[1]).to(self.DEVICE)\n",
    "        # load the pretrained weights from the state dict\n",
    "        self.stgnn.load_state_dict(torch.load(filepath_model, map_location=self.DEVICE))\n",
    "        self.stgnn.eval()\n",
    "\n",
    "    def run_model(self, paths_inputs: list):\n",
    "        \"\"\"creates feature matrix from raw data, does data processing and finally makes predictions\"\"\"\n",
    "        \n",
    "        # create the feature matrix from list of txt files\n",
    "        self.F = self.create_feature_matrix(paths_inputs)\n",
    "        \n",
    "        # performs data transformations: correct shape, normalization and reducing graph\n",
    "        self.process_data()\n",
    "        \n",
    "        # makes predictions using pretrained model\n",
    "        self.yhat = self.get_outputs()\n",
    "\n",
    "        return self.yhat\n",
    "\n",
    "    def create_feature_matrix(self, paths_inputs: list):\n",
    "        \"\"\"create feature matrix from inputs\"\"\"\n",
    "        # check if seconds of input data supplied is consistent with the pretrained model\n",
    "        paths_inputs = self.check_time_in(paths_inputs)     \n",
    "        \n",
    "        # load txt data to numpy array including the node features\n",
    "        f0 = self.load_txt_to_array(paths_inputs[0])\n",
    "        f1 = self.load_txt_to_array(paths_inputs[1])\n",
    "        F = np.stack((f0, f1))\n",
    "\n",
    "        for txt in paths_inputs[2:]:\n",
    "            f = self.load_txt_to_array(txt)\n",
    "            F = np.vstack((F, f[None, :, :]))\n",
    "\n",
    "        return F\n",
    "\n",
    "    def load_txt_to_array(self, txt: str):\n",
    "        \"\"\"load a txt file as numpy array\"\"\"\n",
    "        if txt[-4:] == \".txt\":          # check if input file is a .txt\n",
    "            self.delete_hashtag(txt)    # call function to delete the hashtag in data\n",
    "            d = np.genfromtxt(txt, delimiter=[1, 20], dtype=[(\"f0\", np.uint8), (\"f1\", object)]) # load txt as np array\n",
    "            d = self.load_features(d)   # load the features\n",
    "        else:\n",
    "            raise SystemExit(\"Filetype unsupported. All input files must be .txt\")\n",
    "        return d.astype('uint8')\n",
    "\n",
    "    def delete_hashtag(self, f: str):\n",
    "        \"\"\"remove the # from the input files, otherwise will stop reading after # at later stage\"\"\"\n",
    "        with open(f, \"rb\") as input_file:\n",
    "            s = input_file.read()\n",
    "            input_file.close()\n",
    "            s = s.replace(b\"#\", b\"\")\n",
    "\n",
    "        with open(f, \"wb\") as output_file:\n",
    "            output_file.write(s)\n",
    "\n",
    "    def load_features(self, d: np.ndarray):\n",
    "        \"\"\"loads the different node features to 1 numpy array\"\"\"\n",
    "        x = d[\"f1\"].astype(\"U\")\n",
    "\n",
    "        w = np.where(np.char.find(x, \"Wall\") > 0, 1, 0)    # 1 if wall at that cell, otherwise 0\n",
    "        c = np.where(np.char.find(x, \"coffee\") > 0, 1, 0)  # 1 if coffee machine at that cell, otherwise 0\n",
    "        ws = np.where(np.char.find(x, \"WS\") > 0, 1, 0)`    # 1 if workstation at that cell, otherwise 0\n",
    "\n",
    "        # first column is human presence, second wall, third coffee, fourth workstation\n",
    "        d = np.stack((d[\"f0\"], w), axis=1)\n",
    "        d = np.concatenate((d, c[:, None]), axis=1)\n",
    "        d = np.concatenate((d, ws[:, None]), axis=1)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def check_time_in(self, paths_inputs: list):\n",
    "        \"\"\"check if input length is correct. If the length is longer, some observations will be dropped.\n",
    "        If the length is too short, raise system exit\"\"\"\n",
    "\n",
    "        if len(paths_inputs) > self.periods_in:\n",
    "            print(\"The model uses a historical time series of length \", self.periods_in, \" but \", len(paths_inputs), \" files were supplied.\")\n",
    "            print(\"First \", self.periods_in-len(paths_inputs), \" will be dropped.\")\n",
    "            paths_inputs = paths_inputs[-5:]\n",
    "        elif len(paths_inputs) < self.periods_in:\n",
    "            raise SystemExit\n",
    "\n",
    "        return paths_inputs\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"normalizes, reshape and reduces the grid\"\"\"\n",
    "\n",
    "        # drop nodes\n",
    "        self.F = np.delete(self.F, self.idx, axis=1)\n",
    "        # reshape\n",
    "        self.F = self.F.transpose((1, 2, 0))\n",
    "        # normalize\n",
    "        if self.normalize == True:\n",
    "            self.normalize_zscore()\n",
    "        # add dimension\n",
    "        self.F = np.expand_dims(self.F, axis=0)\n",
    "\n",
    "    def normalize_zscore(self):\n",
    "        \"\"\"z-score normalization\"\"\"\n",
    "        if self.means is None:\n",
    "            raise SystemExit(\"Normalization is set to True but no means are provided\")\n",
    "        elif self.stdev is None:\n",
    "            raise SystemExit(\"Normalization is set to True but no standard deviations are provided\")\n",
    "        \n",
    "        # perform z-score normalization\n",
    "        self.F = self.F - self.means.reshape(1, -1, 1)\n",
    "        self.F = self.F / self.stdev.reshape(1, -1, 1)\n",
    "\n",
    "    def denormalize(self, arr):\n",
    "        # denormalize the normalized outputs of the results\n",
    "        return np.round((arr * self.stdev[0]) + self.means[0], decimals=5)\n",
    "\n",
    "    def get_outputs(self):\n",
    "        \"\"\"runs the model and yields predictions. Denormalizes predictions and adds back omitted nodes\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # make predictions using pretrained model\n",
    "            # inputs are (F, A) --> feature matrix with the same node information, and adjacency matrix storing the topology of the graph\n",
    "            # model expects both F and A to have same  shape as during training\n",
    "            yhat = self.stgnn(torch.from_numpy(self.F.astype(np.float32)).to(self.DEVICE), self.A.to(self.DEVICE)).cpu().detach()\n",
    "\n",
    "        # denormalize if normalize was set to true\n",
    "        if self.normalize == True:\n",
    "            yhat = self.denormalize(yhat)\n",
    "\n",
    "        # add back omitted nodes\n",
    "        yhat = np.insert(yhat, self.idx[0] - np.arange(len(self.idx[0])), 0, axis=1)\n",
    "\n",
    "        return yhat\n",
    "\n",
    "    def export_as_txt(self, yhat: np.ndarray, path: str = \"\", regression_output: bool = True, classification_output: bool = False, threshold: float = None):\n",
    "        \"\"\"export results to txt files\"\"\"\n",
    "        for i in range(yhat.shape[2]):\n",
    "            if regression_output:\n",
    "                np.savetxt(path + \"heatmap_reg_t\" + str(i+1) + \".txt\", yhat[0, :, i])\n",
    "            if classification_output:\n",
    "                yhat = np.where(yhat > threshold, 1, 0).astype(\"uint8\")\n",
    "                np.savetxt(path + \"heatmap_class_t\" + str(i + 1) + \".txt\", yhat[0, :, i])\n",
    "\n",
    "    def plot_heatmap(self, data, img: str):\n",
    "        \"\"\"plot heatmap from results\"\"\"\n",
    "        data = np.array(np.array_split(data[0], 100))\n",
    "        data = np.rot90(data)\n",
    "\n",
    "        ax = sns.heatmap(data, linewidths=0, square=True, cmap='RdYlGn_r', zorder=2, alpha=0.6, cbar=False)\n",
    "        my_image = mpimg.imread(img)\n",
    "        ax.imshow(my_image, aspect=ax.get_aspect(), extent=ax.get_xlim() + ax.get_ylim(), zorder=1)\n",
    "\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction Inputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the inputs used for the prediction must be defined. \n",
    "\n",
    "- Since z-score normalization is applied to the node features of the training data, the normalization must also be applied to the inputs for the prediction. Therefore, the same **means** and **standard deviations** used to normalize the training data must be provided as inputs. \n",
    "\n",
    "- A numpy array with **indices of the omitted nodes** must be provided. If no nodes are omitted, an empty array can be used. \n",
    "\n",
    "- A numpy array with the **adjacency matrix** must be provided. This adjacency matrix stores the connectivity information between nodes i.e. the topology of the graph. This is used by the model to make the predictions.\n",
    "\n",
    "- The path to the **pretrained weights** of the model.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "means_normalization = np.array([0.10162151, 0.03302132, 0.00182007, 0.04836193])     # means of z-score normalization on training data\n",
    "stdev_normalization = np.array([0.33049121, 0.17869223, 0.04262347, 0.21452985])     # st dev of z-score normalization on training data\n",
    "idx_omitted_nodes = np.load(\"example input files/idx_sim2_100p_5_40.npy\")            # indices of omitted nodes\n",
    "A = np.load(\"example input files/Adj_Matrix_Reduced.npy\")                            # adjacency matrix \n",
    "model_path = \"example input files/state_dict.pth\"                                    # path to file containing model weights\n",
    "\n",
    "# initialize the class\n",
    "# all input parameters must be the same as during training\n",
    "model = STGNN_model(node_features=4, periods_in=5, periods_out=40, normalize=True, means=means_normalization,\n",
    "                        stdev=stdev_normalization, idx=idx_omitted_nodes, adj_reduced=A, filepath_model=model_path)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# list of paths to be used as raw inputs for the prediction\n",
    "# 5 heatmaps supplied because with pretrained model periods_in=5 \n",
    "txts = [\"data/simulation2-100p-100cm/heatmap_08H57m32s.txt\",\n",
    "        \"data/simulation2-100p-100cm/heatmap_08H57m33s.txt\",\n",
    "        \"data/simulation2-100p-100cm/heatmap_08H57m34s.txt\",\n",
    "        \"data/simulation2-100p-100cm/heatmap_08H57m35s.txt\",\n",
    "        \"data/simulation2-100p-100cm/heatmap_08H57m36s.txt\",\n",
    "            ]\n",
    "\n",
    "# make prediction\n",
    "# Yields prediction for the next 40 seconds (defined during training)\n",
    "# Prediction accuracy deteriorates over time and after t+1 appears to yields some steady state prediction\n",
    "# For comprehensive analysis of prediction behaviour/quality, see thesis\n",
    "yhat = model.run_model(paths_inputs=txts)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(yhat.shape)\n",
    "print(yhat)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot heatmap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# plot heatmap of prediction\n",
    "t = 1    # which heatmap to show\n",
    "model.plot_heatmap(yhat[:, :, t].numpy(), img=\"test input files/simu2.png\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# model.export_as_txt(yhat, path=\"data/output/\", regression_output=True, classification_output=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
